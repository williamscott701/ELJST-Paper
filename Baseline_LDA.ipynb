{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os, glob\n",
    "import string\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import joblib, multiprocessing\n",
    "import utils as my_utils\n",
    "\n",
    "from scipy import spatial\n",
    "from collections import Counter\n",
    "from scipy.special import gammaln\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "\n",
    "n_cores = 40\n",
    "max_iter = 20\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = glob.glob(\"datasets/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['datasets/amazon_home_20000_dataset',\n",
    "            'datasets/amazon_movies_20000_dataset',\n",
    "            'datasets/amazon_kindle_20000_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/amazon_home_20000_dataset\n",
      "25\n",
      "0.4383337465298385 -0.08531223307069137 9.18802725162832 -25.891884331923112 -2173212.9174732924 1463.986539001863 0.47966966717239584\n",
      "datasets/amazon_movies_20000_dataset\n",
      "25\n",
      "0.4267714391059502 -0.08131075415720888 11.773661158020987 -23.702728603048254 -2261693.1913748463 2287.3904873118017 0.4377831313408154\n",
      "datasets/amazon_kindle_20000_dataset\n",
      "25\n",
      "0.42244168908962115 -0.06624845654736941 11.451573919364147 -23.0936347053355 -2026871.8148782062 1586.423020144608 0.48521930751520226\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "\n",
    "    dataset = pd.read_pickle(dataset)\n",
    "    n_topics = 5 * dataset.sentiment.unique().shape[0]\n",
    "    print(n_topics)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                                 stop_words=\"english\", max_features=max_features,\n",
    "                                 max_df=max_df, min_df=min_df)\n",
    "\n",
    "    count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "\n",
    "    model = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, n_jobs=n_cores, verbose=0)\n",
    "\n",
    "    dt_distribution = model.fit_transform(count_matrix)\n",
    "\n",
    "    topic_words = {}\n",
    "    for topic, comp in enumerate(model.components_):\n",
    "        word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "        topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "    sample_df = []\n",
    "    for topic, word in topic_words.items():\n",
    "        sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "\n",
    "    print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "          silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "          davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "          my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "          model.score(count_matrix),\n",
    "          model.perplexity(count_matrix),\n",
    "          my_utils.coherence_score2(count_matrix, sample_df, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/amazon_electronics_20000_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(zip(words,np.arange(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=n_topics, max_iter=20, n_jobs=n_cores, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = {}\n",
    "for topic, comp in enumerate(model.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "sample_df = []\n",
    "for topic, word in topic_words.items():\n",
    "    sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "dt_distribution = model.transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "      silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "      model.score(count_matrix),\n",
    "      model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000))\n",
    "# print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "# print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "# print(\"Perplexity:\", model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, k, 3000))\n",
    "#     print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "#     print(\"Perplexity:\", model.perplexity(count_matrix))\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "    \n",
    "# #     print my_utils.coherence_score(count_matrix, sample_df, vocabulary), \"\\t\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)), \"\\t\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_evaluations_multi(model):\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     h_score =  my_utils.get_hscore_multi(dt_distribution, count_matrix, k)\n",
    "#     likelihood =  model.score(count_matrix)\n",
    "#     perplexity = model.perplexity(count_matrix)\n",
    "#     coherance_score = my_utils.coherence_score(count_matrix, sample_df, vocabulary)\n",
    "#     silhouette = silhouette_score(count_matrix, dt_distribution.argmax(axis=1))\n",
    "#     return [h_score, likelihood, perplexity, coherance_score, silhouette]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"H-Score:\", my_utils.get_hscore(dt_distribution, count_matrix, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(dt_distribution)\n",
    "\n",
    "# X_embedded.shape\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter([i[0] for i in X_embedded], [i[1] for i in X_embedded], c=dt_distribution.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
