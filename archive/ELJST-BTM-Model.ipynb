{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial, sparse\n",
    "from scipy.stats import chi2\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.externals import joblib \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import gzip\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import LDA_ELJST as lda\n",
    "#import ELJST_script_unigram as lda\n",
    "#import LJST_script_BTM as lda\n",
    "import ELJST_script_BTM as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"musical_review.csv\",engine='python') #pd.read_csv('stf_50k.csv')\n",
    "dataset.sentiment_score = dataset.sentiment_score.astype(int)\n",
    "dataset[\"clean_sentence\"] = dataset[\"clean_sentence\"].apply(lambda x: \" \".join([st.stem(i) for i in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>wordlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>5</td>\n",
       "      <td>much write exactli suppo filter pop sound reco...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>Jake</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>5</td>\n",
       "      <td>product exactli quit affordablei realiz doubl ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A2IBPI20UZIR0U  1384719342   \n",
       "1  A14VAT5EAX3D9S  1384719342   \n",
       "\n",
       "                                       reviewerName   helpful  \\\n",
       "0  cassandra tu \"Yeah, well, that's just like, u...    [0, 0]   \n",
       "1                                              Jake  [13, 14]   \n",
       "\n",
       "                                          reviewText  overall summary  \\\n",
       "0  Not much to write about here, but it does exac...        5    good   \n",
       "1  The product does exactly as it should and is q...        5    Jake   \n",
       "\n",
       "   unixReviewTime   reviewTime  sentiment_score  \\\n",
       "0      1393545600  02 28, 2014                5   \n",
       "1      1363392000  03 16, 2013                5   \n",
       "\n",
       "                                      clean_sentence  wordlen  \n",
       "0  much write exactli suppo filter pop sound reco...       21  \n",
       "1  product exactli quit affordablei realiz doubl ...       45  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5856\n",
       "4    1658\n",
       "3     605\n",
       "2     189\n",
       "1     176\n",
       "Name: sentiment_score, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sentiment_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 10\n",
    "lambda_param = 1\n",
    "N_SENTIMENT = 5\n",
    "n_topics = 5\n",
    "alpha = 0.1/n_topics * np.ones(n_topics)\n",
    "gamma = 10\n",
    "gamma = [gamma/(n_topics*N_SENTIMENT)]*N_SENTIMENT\n",
    "beta = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda2 = LatentDirichletAllocation(n_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [01:21, 4923.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_index = loadGloveModel('C:/Users/asengup6/Documents/models/glove.6B.{}d.txt'.format(embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJST Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.SentimentLDAGibbsSampler(n_topics, alpha, beta, gamma, numSentiments=N_SENTIMENT, minlabel = 0, \n",
    "                                       maxlabel = 5, SentimentRange = 5, max_df = .5, min_df = 5, lambda_param = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler._initialize_(reviews=list(dataset.clean_sentence), labels=list(dataset.sentiment_score), unlabeled_reviews=[],skipgramwindow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8484, 2723)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordOccuranceMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nword_similarity ={}\\nwords_without_emb = 0\\ncutoff = .5\\n\\nfor i in tqdm(range(sampler.wordOccuranceMatrix.shape[0])):\\n    words_embeddings = []\\n    for val in list(np.where(sampler.wordOccuranceMatrix[i] > 0)[0]):\\n        word = sampler.vectorizer.get_feature_names()[val]\\n\\n        if len(word.split()) == 1:\\n            emb = embeddings_index.get(word,np.array([0]*embedding_dim))\\n        else:\\n            emb = np.array([0]*embedding_dim)\\n            count = 0\\n            for w in word.split():\\n                if w in embeddings_index:\\n                    count += 1\\n                emb = emb + embeddings_index.get(w,np.array([0]*embedding_dim))\\n            if count != 0:\\n                emb = emb/count\\n\\n        words_embeddings.append(emb)\\n        \\n    words_embeddings = cosine_similarity(np.array(words_embeddings))    \\n    words_embeddings = words_embeddings > cutoff\\n    words_embeddings = words_embeddings.astype(int)\\n    \\n    word_similarity[i] = words_embeddings\\n    \\npickle.dump(word_similarity,open('word_similarity_amazon_musical5_cutoff.pkl','wb'))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "word_similarity ={}\n",
    "words_without_emb = 0\n",
    "cutoff = .5\n",
    "\n",
    "for i in tqdm(range(sampler.wordOccuranceMatrix.shape[0])):\n",
    "    words_embeddings = []\n",
    "    for val in list(np.where(sampler.wordOccuranceMatrix[i] > 0)[0]):\n",
    "        word = sampler.vectorizer.get_feature_names()[val]\n",
    "\n",
    "        if len(word.split()) == 1:\n",
    "            emb = embeddings_index.get(word,np.array([0]*embedding_dim))\n",
    "        else:\n",
    "            emb = np.array([0]*embedding_dim)\n",
    "            count = 0\n",
    "            for w in word.split():\n",
    "                if w in embeddings_index:\n",
    "                    count += 1\n",
    "                emb = emb + embeddings_index.get(w,np.array([0]*embedding_dim))\n",
    "            if count != 0:\n",
    "                emb = emb/count\n",
    "\n",
    "        words_embeddings.append(emb)\n",
    "        \n",
    "    words_embeddings = cosine_similarity(np.array(words_embeddings))    \n",
    "    words_embeddings = words_embeddings > cutoff\n",
    "    words_embeddings = words_embeddings.astype(int)\n",
    "    \n",
    "    word_similarity[i] = words_embeddings\n",
    "    \n",
    "pickle.dump(word_similarity,open('word_similarity_amazon_musical5_cutoff.pkl','wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2723/2723 [00:00<00:00, 26767.72it/s]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = []\n",
    "\n",
    "for word in tqdm(sampler.vectorizer.get_feature_names()):\n",
    "    emb = embeddings_index.get(word,np.array([0]*embedding_dim))\n",
    "    word_embeddings.append(emb)\n",
    "    \n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_similarity = cosine_similarity(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.03067062, -0.04051867, ...,  0.06807656,\n",
       "        -0.02699289,  0.16327779],\n",
       "       [-0.03067062,  1.        , -0.06469126, ...,  0.02309567,\n",
       "        -0.06563686,  0.04846905],\n",
       "       [-0.04051867, -0.06469126,  1.        , ...,  0.08963072,\n",
       "         0.01120404,  0.05127578],\n",
       "       ...,\n",
       "       [ 0.06807656,  0.02309567,  0.08963072, ...,  1.        ,\n",
       "        -0.01658845,  0.19891084],\n",
       "       [-0.02699289, -0.06563686,  0.01120404, ..., -0.01658845,\n",
       "         1.        ,  0.05814918],\n",
       "       [ 0.16327779,  0.04846905,  0.05127578, ...,  0.19891084,\n",
       "         0.05814918,  1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = .5\n",
    "\n",
    "word_similarity = word_similarity > cutoff\n",
    "word_similarity = word_similarity.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                        | 1/10 [08:40<1:18:06, 520.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████                                                                | 2/10 [17:56<1:10:50, 531.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████                                                        | 3/10 [26:52<1:02:09, 532.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▊                                                 | 4/10 [35:52<53:28, 534.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 5/10 [44:42<44:26, 533.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 6 of 10\n"
     ]
    }
   ],
   "source": [
    "sampler.run(reviews=list(dataset.clean_sentence), labels=list(dataset.sentiment_score), unlabeled_reviews=[], similar_words = word_similarity, mrf = True, maxIters=maxiter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.19237411e-01 9.44232487e-05 3.07533036e-04 4.08698037e-04\n",
      "  8.79083797e-01]\n",
      " [2.54317927e-05 2.30507799e-06 1.01918748e-05 2.79899808e-05\n",
      "  1.57428028e-04]\n",
      " [8.43027294e-06 8.53239889e-06 1.54516468e-05 1.28547451e-05\n",
      "  2.04802440e-04]\n",
      " [2.18392528e-05 3.45789587e-05 3.92078662e-09 4.40688525e-06\n",
      "  1.97935844e-04]\n",
      " [2.07957036e-05 1.02152325e-05 2.21329706e-05 5.81800780e-06\n",
      "  7.69927485e-05]] [[1.11748361 1.00004079 1.00004079 1.00004079 1.00004079]\n",
      " [1.         1.         1.00004079 1.00004079 1.11748361]\n",
      " [1.11748361 1.         1.         1.11743803 1.00004079]\n",
      " [1.11748361 1.11748361 1.00004079 1.11743803 1.        ]\n",
      " [1.00004079 1.         1.11743803 1.00004079 1.11748361]] [[1.11622128 1.0014951  1.0014951  1.0014951  1.0014951 ]\n",
      " [1.         1.         1.0014951  1.0014951  1.11622128]\n",
      " [1.11622128 1.         1.         1.11455491 1.0014951 ]\n",
      " [1.11622128 1.11622128 1.0014951  1.11455491 1.        ]\n",
      " [1.0014951  1.         1.11455491 1.0014951  1.11622128]]\n",
      "[[1.44269592e-01 9.17310346e-05 2.98764594e-04 3.97045158e-04\n",
      "  8.54019188e-01]\n",
      " [2.46687863e-05 2.23592088e-06 9.90128204e-06 2.71919250e-05\n",
      "  1.90477782e-04]\n",
      " [1.02000877e-05 8.27640927e-06 1.49880654e-05 1.55295615e-05\n",
      "  1.98963073e-04]\n",
      " [2.64240903e-05 4.18383145e-05 3.80899639e-09 5.32387029e-06\n",
      "  1.91997359e-04]\n",
      " [2.02027725e-05 9.90875440e-06 2.67384009e-05 5.65212362e-06\n",
      "  9.31562703e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.44269592e-01, 9.17310346e-05, 2.98764594e-04, 3.97045158e-04,\n",
       "        8.54019188e-01],\n",
       "       [2.46687863e-05, 2.23592088e-06, 9.90128204e-06, 2.71919250e-05,\n",
       "        1.90477782e-04],\n",
       "       [1.02000877e-05, 8.27640927e-06, 1.49880654e-05, 1.55295615e-05,\n",
       "        1.98963073e-04],\n",
       "       [2.64240903e-05, 4.18383145e-05, 3.80899639e-09, 5.32387029e-06,\n",
       "        1.91997359e-04],\n",
       "       [2.02027725e-05, 9.90875440e-06, 2.67384009e-05, 5.65212362e-06,\n",
       "        9.31562703e-05]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.conditionalDistribution(0,0,word_similarity,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,  76.,   0.],\n",
       "       [107.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 192.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0., 110.,   0.,   0.],\n",
       "       [  0., 170.,   0.,   0.,   0.],\n",
       "       [  0., 121.,   1.,   0.,   0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.n_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.62812089e-04, 2.62812089e-04, 2.62812089e-04, 9.98948752e-01,\n",
       "        2.62812089e-04],\n",
       "       [9.99253035e-01, 1.86741363e-04, 1.86741363e-04, 1.86741363e-04,\n",
       "        1.86741363e-04],\n",
       "       [1.04112441e-04, 1.04112441e-04, 1.04112441e-04, 9.99583550e-01,\n",
       "        1.04112441e-04],\n",
       "       ...,\n",
       "       [1.81653043e-04, 1.81653043e-04, 9.99273388e-01, 1.81653043e-04,\n",
       "        1.81653043e-04],\n",
       "       [1.17577895e-04, 9.99529688e-01, 1.17577895e-04, 1.17577895e-04,\n",
       "        1.17577895e-04],\n",
       "       [1.63800164e-04, 9.91154791e-01, 8.35380835e-03, 1.63800164e-04,\n",
       "        1.63800164e-04]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.dt_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,  76.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0., 105.,   0.,   2.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   5., 111.,  76.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   5.,   5., 100.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 170.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 120.,   0.,   1.,   0.],\n",
       "        [  0.,   0.,   0.,   1.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.n_dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.04166667e-03, 9.90625000e-01, 1.04166667e-03, 1.04166667e-03,\n",
       "         6.25000000e-03],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01]],\n",
       "\n",
       "       [[7.42115028e-04, 7.42115028e-04, 9.74768089e-01, 7.42115028e-04,\n",
       "         2.30055659e-02],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01]],\n",
       "\n",
       "       [[1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [4.14937759e-04, 2.63485477e-02, 5.76141079e-01, 3.94605809e-01,\n",
       "         2.48962656e-03],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [7.22021661e-04, 7.22021661e-04, 4.58483755e-02, 4.58483755e-02,\n",
       "         9.06859206e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 1.00000000e-01,\n",
       "         6.00000000e-01]],\n",
       "\n",
       "       [[1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01],\n",
       "        [4.68384075e-04, 9.95784543e-01, 4.68384075e-04, 2.81030445e-03,\n",
       "         4.68384075e-04],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01]],\n",
       "\n",
       "       [[1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01],\n",
       "        [6.56814450e-04, 9.85878489e-01, 6.56814450e-04, 1.21510673e-02,\n",
       "         6.56814450e-04],\n",
       "        [4.44444444e-02, 4.44444444e-02, 4.44444444e-02, 8.22222222e-01,\n",
       "         4.44444444e-02],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01],\n",
       "        [1.00000000e-01, 1.00000000e-01, 1.00000000e-01, 6.00000000e-01,\n",
       "         1.00000000e-01]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.dts_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=5, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.fit(sampler.wordOccuranceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = lda2.transform(sampler.wordOccuranceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9569813 , 0.01060561, 0.01078143, 0.0108171 , 0.01081457],\n",
       "       [0.70973751, 0.00638399, 0.27112138, 0.00639147, 0.00636564],\n",
       "       [0.82370132, 0.04960377, 0.11618825, 0.00526918, 0.00523747],\n",
       "       ...,\n",
       "       [0.0065411 , 0.26133197, 0.00662374, 0.00664748, 0.71885572],\n",
       "       [0.00533946, 0.26301878, 0.0735857 , 0.00539578, 0.65266028],\n",
       "       [0.09207113, 0.15719974, 0.0055186 , 0.00553124, 0.73967929]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1714, 1: 1601, 2: 1762, 3: 1606, 4: 1801})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2275, 1: 2152, 2: 604, 3: 1380, 4: 2073})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dt.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8484, 8484)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_doc = cosine_distances(sampler.wordOccuranceMatrix)\n",
    "cosine_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.61745397, 0.78932476, ..., 0.91507922, 0.9729631 ,\n",
       "        0.97745826],\n",
       "       [0.61745397, 0.        , 0.81578748, ..., 0.9783426 , 0.97931428,\n",
       "        0.91376747],\n",
       "       [0.78932476, 0.81578748, 0.        , ..., 0.93866044, 0.96094167,\n",
       "        0.96743552],\n",
       "       ...,\n",
       "       [0.91507922, 0.9783426 , 0.93866044, ..., 0.        , 0.61427286,\n",
       "        0.47932069],\n",
       "       [0.9729631 , 0.97931428, 0.96094167, ..., 0.61427286, 0.        ,\n",
       "        0.69283273],\n",
       "       [0.97745826, 0.91376747, 0.96743552, ..., 0.47932069, 0.69283273,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0030386771795631695"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),sampler.dt_distribution.argmax(axis=1),metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0043106057580392775"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),dt.argmax(axis=1),metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  score = (intra_dists[:, None] + intra_dists) / centroid_distances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.8862831735247"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix,sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  score = (intra_dists[:, None] + intra_dists) / centroid_distances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.588950650541125"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix,dt.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.049816459458132"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, list(sampler.getTopKWords(5).values()), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0466363716258515"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_utils.get_hscore(sampler.dt_distribution[:1000],sampler.wordOccuranceMatrix[:1000],n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    output = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        output.append([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        #print(message)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = sampler.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.775766438285192"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, print_top_words(lda2, tf_feature_names, 5), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19994489337908955"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_utils.get_hscore(dt[:1000],sampler.wordOccuranceMatrix[:1000],n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_matrix, _, vocabulary, words = my_utils.processReviews(dataset['clean_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06756489]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embeddings_index[\"turkey\"].reshape(1, -1),embeddings_index[\"fridge\"].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10478221]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embeddings_index[\"spaceship\"].reshape(1, -1),embeddings_index[\"fridge\"].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
