{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial, sparse\n",
    "from scipy.stats import chi2\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.externals import joblib \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import os\n",
    "import imp\n",
    "import gzip\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import LDA_ELJST as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as my_utils\n",
    "from sentiment import SentimentAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"stackoverflow_fasttext_with_stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_questions = pd.read_csv(\"nongit_resources/Questions.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"resources/data_stackoverflow_5kanswers_pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns={'cleaned_body': 8, 'clean': 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dataset['sent_score_quant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix_, tfidf_matrix_, vocabulary, words = my_utils.processReviews(dataset[9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix_.shape, tfidf_matrix_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barren = np.where(count_matrix_.sum(1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix, tfidf_matrix, texts, ratings, ids = [], [], [], [], []\n",
    "\n",
    "texts_ =  dataset['body_text'].values.tolist()\n",
    "ratings_ = dataset['sent_score_quant'].values.tolist()\n",
    "ids_ = dataset['Id'].values.tolist()\n",
    "\n",
    "for idx, (i, j, k, l, m) in enumerate(zip(count_matrix_, tfidf_matrix_, texts_, ratings_, ids_)):\n",
    "    if idx not in barren:\n",
    "        count_matrix.append(i.tolist())\n",
    "        tfidf_matrix.append(j.tolist())\n",
    "        texts.append(k)\n",
    "        ratings.append(l)\n",
    "        ids.append(m)\n",
    "        \n",
    "count_matrix = np.array(count_matrix)\n",
    "tfidf_matrix = np.array(tfidf_matrix)\n",
    "texts = np.array(texts)\n",
    "ratings = np.array(ratings)\n",
    "ids = np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Edge_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"resources/docs_edges_stackoverflow_glove_5kanswers.pickle\",\"rb\")\n",
    "docs_edges = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_edges = np.delete(docs_edges, barren).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for i in docs_edges:\n",
    "    for j in i:\n",
    "        try:\n",
    "            edge_dict[j[0]] += [j[1]]\n",
    "            edge_dict[j[1]] += [j[0]]\n",
    "        except:\n",
    "            edge_dict[j[0]] = [j[1]]\n",
    "            edge_dict[j[1]] = [j[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in edge_dict.keys():\n",
    "    edge_dict[i] = list(set(edge_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 20\n",
    "lambda_param = 1.0\n",
    "N_ITERATAIONS = 5\n",
    "N_SENTIMENT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = str(datetime.datetime.now()) + \"_\" + dataset_name\n",
    "os.mkdir(\"dumps/\"+folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_grid = [5, 10, 25, 30,  35, 45, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multiprocessing_func(k):\n",
    "#     sampler = lda.LdaSampler(n_sentiment = N_SENTIMENT, n_topics=k, lambda_param=lambda_param)\n",
    "#     sampler.store_data(count_matrix, ratings, words, vocabulary, docs_edges, edge_dict, ids)\n",
    "\n",
    "#     print(\"Running Sampler...\", k)\n",
    "#     for i in range(N_ITERATAIONS):\n",
    "#         print(datetime.datetime.now().time(), \" iteration:\", i, \"k\", k)\n",
    "#         sampler.run(count_matrix, ratings, edge_dict, maxiter=maxiter)\n",
    "#         joblib.dump(sampler, \"dumps/\" + folder_name + \"/sampler_\" + dataset_name + \"_n_topics_\" + str(k) + \"_maxiter_\"\n",
    "#                     + str(maxiter) + \"_iter_\" + str(i+1) +\"_in_\" + str(N_ITERATAIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool.map(multiprocessing_func, topics_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name = \"2019-08-16 23:49:40.458236_stackoverflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names = ['sampler_stackoverflow_n_topics_5_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_10_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_25_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_30_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_35_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_45_maxiter_20_iter_5_in_5',\n",
    "#                 'sampler_stackoverflow_n_topics_50_maxiter_20_iter_5_in_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "# def geometric_mean(x):\n",
    "#     x = [i for i in x if i!=0]\n",
    "#     return scipy.stats.mstats.gmean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, k in zip(dataset_names, topics_grid):\n",
    "#     sampler = joblib.load(\"dumps/\" + folder_name + \"/\" + i)\n",
    "#     data = pd.DataFrame()\n",
    "\n",
    "#     data['text'] = texts\n",
    "#     data['sent'] = ratings\n",
    "\n",
    "#     count_matrix = sampler.get_count_matrix()\n",
    "#     binary_mat = (count_matrix > 0).astype(int)\n",
    "#     word_freq = binary_mat.sum(axis=0)\n",
    "\n",
    "#     df_matrix = np.matmul(binary_mat, np.diag(word_freq))\n",
    "#     df_matrix = df_matrix * 1.0/df_matrix.shape[0]\n",
    "\n",
    "#     document_topic_entropy = scipy.stats.entropy(sampler.theta().transpose())\n",
    "#     data['document_topic_entropy'] = document_topic_entropy\n",
    "\n",
    "#     word_len = count_matrix.sum(axis=1)\n",
    "#     data['word_len'] = word_len\n",
    "\n",
    "#     data['document_popularity'] = np.array([geometric_mean(x) for x in df_matrix.tolist()])\n",
    "\n",
    "#     normalized_dts = sampler.pi() * sampler.theta()[:,:,np.newaxis]\n",
    "#     normalized_dts /= normalized_dts.sum(axis=-1)[:,:,np.newaxis]\n",
    "\n",
    "#     document_topic_sentiment_crossentropy = np.array([[scipy.stats.entropy(j) for j in i] for i in normalized_dts])\n",
    "#     document_ts_entropy_min = document_topic_sentiment_crossentropy.min(axis=1)\n",
    "#     document_ts_entropy_mean = document_topic_sentiment_crossentropy.mean(axis=1)\n",
    "#     document_ts_entropy_var = np.sqrt(document_topic_sentiment_crossentropy.var(axis=1))\n",
    "\n",
    "#     data['document_ts_entropy_mean'] = document_ts_entropy_mean\n",
    "#     data['document_ts_entropy_min'] = document_ts_entropy_min\n",
    "#     data['document_ts_entropy_var'] = document_ts_entropy_var\n",
    "\n",
    "#     data['num_edges'] = [len(i) for i in sampler.docs_edges]\n",
    "#     data['Id'] = sampler.answer_id\n",
    "\n",
    "#     topic_distances = 1/cosine_similarity(sampler.theta().transpose())\n",
    "#     rao_score = []\n",
    "#     for i in range(sampler.theta().shape[0]):\n",
    "#         temp = np.outer(sampler.theta()[i],sampler.theta()[i]) * topic_distances\n",
    "#         rao_score.append(0.5*temp.sum()+0.5*temp.trace())\n",
    "#     rao_score = np.array(rao_score)\n",
    "\n",
    "#     data['rao'] = rao_score\n",
    "\n",
    "#     merged = data.merge(dataset, on='Id', how='inner')\n",
    "\n",
    "#     dataset_questions = dataset_questions.rename(columns={'Id': 'ParentId'})\n",
    "\n",
    "#     merged_questions = merged.merge(dataset_questions, on='ParentId', how='inner')\n",
    "\n",
    "#     merged_questions.drop(columns=['text', 'sent', 8, 'CreationDate_y', 'CreationDate_x', 'Score_y', \n",
    "#                                    'Body_x']).rename(columns={'answererID': 'answerID', 'OwnerUserId_x': 'OwnerUserId', \n",
    "#                                                               'Score_x': 'answer_score', 'body_text': 'answer', 9: 'answer_clean', 'OwnerUserId_y': 'QuestionID', 'Title': 'Questions_Title', 'Body_y': 'Question_body'}).reset_index().to_pickle(\"dumps/\" + folder_name + \"/eval_tests_stackoverflow_pd_topics_\" + str(k), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install numpy num2words nltk pandas Observations gensim\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install -U textblob\n",
    "# # !python -m textblob.download_corpora\n",
    "# import nltk\n",
    "# nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_matrix = scipy.sparse.load_npz(\"resources/df_stackoverflow_5kanswers.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = joblib.load(\"dumps/\" + folder_name + \"/\" + i)\n",
    "# data = pd.DataFrame()\n",
    "\n",
    "# data['text'] = text_folds[idx]\n",
    "# data['sent'] = sent_folds[idx]\n",
    "\n",
    "# test_set = sampler.test_sentiment.shape[0]\n",
    "# count_matrix = sampler.get_count_matrix()\n",
    "# binary_mat = (count_matrix > 0).astype(int)\n",
    "# word_freq = binary_mat.sum(axis=0)\n",
    "\n",
    "# df_matrix = np.matmul(binary_mat, np.diag(word_freq))\n",
    "# df_matrix = df_matrix * 1.0/df_matrix.shape[0]\n",
    "\n",
    "# document_topic_entropy = scipy.stats.entropy(sampler.theta().transpose())\n",
    "# data['document_topic_entropy'] = document_topic_entropy\n",
    "\n",
    "# word_len = count_matrix.sum(axis=1)\n",
    "# data['word_len'] = word_len\n",
    "\n",
    "# document_popularity = np.true_divide(df_matrix.sum(1),(df_matrix != 0).sum(1)) \n",
    "# #scipy.stats.mstats.gmean(idf_matrix,axis=1)\n",
    "# data['document_popularity'] = document_popularity\n",
    "\n",
    "# normalized_dts = sampler.pi() * sampler.theta()[:,:,np.newaxis]\n",
    "# normalized_dts /= normalized_dts.sum(axis=-1)[:,:,np.newaxis]\n",
    "\n",
    "# document_topic_sentiment_crossentropy = np.array([[scipy.stats.entropy(j) for j in i] for i in normalized_dts])\n",
    "# document_ts_entropy_min = document_topic_sentiment_crossentropy.min(axis=1)\n",
    "# document_ts_entropy_mean = document_topic_sentiment_crossentropy.mean(axis=1)\n",
    "# document_ts_entropy_var = np.sqrt(document_topic_sentiment_crossentropy.var(axis=1))\n",
    "\n",
    "# data['document_ts_entropy_mean'] = document_ts_entropy_mean\n",
    "# data['document_ts_entropy_min'] = document_ts_entropy_min\n",
    "# data['document_ts_entropy_var'] = document_ts_entropy_var\n",
    "\n",
    "# data['num_edges'] = [len(i) for i in sampler.docs_edges]\n",
    "# data['answererID'] = sampler.answer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = pd.concat(csv_dumps).merge(dataset, on='answererID', how='inner')\n",
    "\n",
    "# merged.drop(columns=['text', 'sent', 8]).reset_index().to_pickle(\"dumps/\" + folder_name + \"/eval_tests_amazon_QA_Electronics_pd\", protocol=2)\n",
    "\n",
    "# merged.shape\n",
    "\n",
    "# merged_questions = merged.merge(dataset_questions, on='ParentId', how='inner')\n",
    "\n",
    "# merged_questions.keys()\n",
    "\n",
    "# merged_questions\n",
    "\n",
    "# merged_questions.drop(columns=['text', 'sent', 8, 'CreationDate_y', 'CreationDate_x', 'Score_y', 'Body_x']).rename(columns={'answererID': 'answerID', 'OwnerUserId_x': 'OwnerUserId', 'Score_x': 'answer_score', 'body_text': 'answer', 9: 'answer_clean', 'OwnerUserId_y': 'QuestionID', 'Title': 'Questions_Title', 'Body_y': 'Question_body'}).reset_index().to_pickle(\"dumps/\" + folder_name + \"/eval_tests_stackoverflow_answers_pd\", protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents_val = (dataset['sent1'].values*1000).astype(int)\n",
    "# freq = []\n",
    "# val = []\n",
    "# for i in p:\n",
    "#     if i != 0 and i > -200 and i < 200:\n",
    "#         freq.append(p[i])\n",
    "#         val.append(i)\n",
    "\n",
    "# plt.plot(val, freq)\n",
    "\n",
    "# len(sents_val)/5\n",
    "\n",
    "# sents_range = []\n",
    "# for i in sents_val:\n",
    "#     if i < -10:\n",
    "#         sents_range.append(1)\n",
    "#     elif i < 1:\n",
    "#         sents_range.append(2)\n",
    "#     elif i < 22:\n",
    "#         sents_range.append(3)\n",
    "#     elif i < 52:\n",
    "#         sents_range.append(4)\n",
    "#     else:\n",
    "#         sents_range.append(5)\n",
    "\n",
    "# Counter(sents_range)\n",
    "\n",
    "# ratings = np.array(sents_range)\n",
    "\n",
    "# dataset['sent_range'] = sents_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import floor\n",
    "# import nltk\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# tagger = PerceptronTagger()\n",
    "\n",
    "# def compute_sentiment(sentence):\n",
    "#     taggedsentence = []\n",
    "#     sent_score = []\n",
    "#     taggedsentence.append(tagger.tag(sentence.split()))\n",
    "#     wnl = nltk.WordNetLemmatizer()\n",
    "#     for idx, words in enumerate(taggedsentence):\n",
    "#         for idx2, t in enumerate(words):\n",
    "#             newtag = ''\n",
    "#             lemmatizedsent = wnl.lemmatize(t[0])\n",
    "#             if t[1].startswith('NN'):\n",
    "#                 newtag = 'n'\n",
    "#             elif t[1].startswith('JJ'):\n",
    "#                 newtag = 'a'\n",
    "#             elif t[1].startswith('V'):\n",
    "#                 newtag = 'v'\n",
    "#             elif t[1].startswith('R'):\n",
    "#                 newtag = 'r'\n",
    "#             else:\n",
    "#                 newtag = ''\n",
    "#             if (newtag != ''):\n",
    "#                 synsets = list(swn.senti_synsets(lemmatizedsent, newtag))\n",
    "#                 score = 0.0\n",
    "#                 if (len(synsets) > 0):\n",
    "#                     for syn in synsets:\n",
    "#                         score += syn.pos_score() - syn.neg_score()\n",
    "#                     sent_score.append(score / len(synsets))\n",
    "#         if (len(sent_score)==0 or len(sent_score)==1):\n",
    "#             return (float(0.0))\n",
    "#         else:\n",
    "#             return (sum([word_score for word_score in sent_score]) / (len(sent_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadGloveModel(gloveFile):\n",
    "#     print(\"Loading Glove Model\")\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#         model[word] = embedding\n",
    "#     print(\"Done.\",len(model),\" words loaded!\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = loadGloveModel(\"nongit_resources/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/electronics.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# edges_threshold = 0.8\n",
    "# docs_edges, ignored, taken, count = [], [], [], 0\n",
    "# for idx, doc in enumerate(dataset[8].values):\n",
    "#     edges = []\n",
    "#     print(idx)\n",
    "#     for i in doc:\n",
    "#         for j in doc:\n",
    "#             if i != j:\n",
    "#                 try:\n",
    "#                     a = embeddings_index[i]\n",
    "#                     b = embeddings_index[j]\n",
    "#                     if get_cosine(a, b) > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "#                         edges.append((vocabulary[i], vocabulary[j]))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         embeddings_index[i]\n",
    "#                         taken.append(i)\n",
    "#                     except:\n",
    "#                         ignored.append(i)\n",
    "#                     try:\n",
    "#                         embeddings_index[j]\n",
    "#                     except:\n",
    "#                         ignored.append(j)\n",
    "#                         taken.append(j)\n",
    "#                     pass\n",
    "#     docs_edges.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/docs_edges_\" + dataset_name + \"_5k_fasttext_trained.pickle\",\"wb\")\n",
    "# pickle.dump(docs_edges, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence wise\n",
    "# dataset = dataset[['asin', 'helpful', 'overall', 'reviewText']]\n",
    "# dataset['n_words'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x)))\n",
    "# dataset['sentences'] = dataset['reviewText'].apply(lambda x: [i.strip() for i in x.split(\".\")])\n",
    "# dataset['sentence_word_density'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x))/ len(x.split(\".\")))\n",
    "# dataset.to_csv(\"reviews_Musical_Instruments_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# edge_dict, ignored, taken, count = {}, [], [], 0\n",
    "# for idxi, i in enumerate(vocabulary.keys()):\n",
    "#     print(idxi)\n",
    "#     for idxj, j in enumerate(vocabulary.keys()):\n",
    "#         if i != j:\n",
    "#             try:\n",
    "#                 a = embeddings_index[i]\n",
    "#                 b = embeddings_index[j]\n",
    "#                 if get_cosine(a, b) > edges_threshold:\n",
    "#                     try:\n",
    "#                         edge_dict[vocabulary[i]] += [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] += [vocabulary[i]]\n",
    "#                     except:\n",
    "#                         edge_dict[vocabulary[i]] = [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] = [vocabulary[i]]\n",
    "#             except:\n",
    "#                 try:\n",
    "#                     embeddings_index[i]\n",
    "#                     taken.append(i)\n",
    "#                 except:\n",
    "#                     ignored.append(i)\n",
    "#                 try:\n",
    "#                     embeddings_index[j]\n",
    "#                 except:\n",
    "#                     ignored.append(j)\n",
    "#                     taken.append(j)\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {}\n",
    "\n",
    "# for idx, i in enumerate(dataset[8].values):\n",
    "#     print(idx)\n",
    "#     for j in i:\n",
    "#         try:\n",
    "#             df[j] += [idx]\n",
    "#         except:\n",
    "#             df[j] = [idx]\n",
    "\n",
    "# for i in df.keys():\n",
    "#     df[i] = len(list(set(df[i])))\n",
    "\n",
    "# df_vector = []\n",
    "# for i in dataset[8].values:\n",
    "#     d = [0]*len(vocabulary.keys())\n",
    "#     for j in i:\n",
    "#         if j in vocabulary.keys():\n",
    "#             d[vocabulary[j]] = df[j]\n",
    "#     df_vector.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csr = sparse.csr_matrix(np.array(df_vector))\n",
    "# scipy.sparse.save_npz('resources/df_amazon_baby_sparse_200questions.npz', csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(scipy.sparse.load_npz('resources/df_amazon_baby_sparse_200questions.npz').todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = parse(\"nongit_resources/reviews_Electronics_5.json.gz\")\n",
    "# dataset = pd.DataFrame(list(dataset))\n",
    "# dataset = dataset.head(N_docs)\n",
    "# dataset.to_pickle(\"resources/reviews_Electronics_5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
