{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_embedding import BertEmbedding\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "from transformers import *\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "import imp, gzip\n",
    "import pickle, nltk\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(i):\n",
    "    t = np.where(i>0)[0]\n",
    "    comb = combinations(t, 2)\n",
    "    embeds = {j:[] for j in t}\n",
    "\n",
    "    for p, q in comb:\n",
    "        if word_similarity[p][q]:\n",
    "            embeds[p] += [q]\n",
    "            embeds[q] += [p]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in tqdm(parse(path)):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['text'] = my_utils.preprocess(df['text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_transformers(text):\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    if embedding_name == 'bert':\n",
    "        results = bert_embedding(sentence)\n",
    "        embed_vecs = np.array([i[1][0] for i in results])\n",
    "    else:\n",
    "        embed_vecs = elmo.embed_sentence(sentence)[2]\n",
    "\n",
    "    l = np.array(list(set(sentence).intersection(words)))\n",
    "\n",
    "    pp = np.array([i[1] for i in nltk.pos_tag(l)])\n",
    "    pp[pp=='JJ'] = 1\n",
    "    pp[pp=='JJR'] = 1\n",
    "    pp[pp=='JJS'] = 1\n",
    "    pp[pp=='NN'] = 1\n",
    "    pp[pp=='NNS'] = 1\n",
    "    pp[pp=='NNP'] = 1\n",
    "    pp[pp=='NNPS'] = 1\n",
    "    pp[pp!='1'] = 0\n",
    "    pp = pp.astype(int)\n",
    "\n",
    "    l = l[pp==1]\n",
    "\n",
    "    word_embeddings = np.array([embed_vecs[sentence.index(i)] for i in l])\n",
    "\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    remove = np.where(word_similarity == 1)\n",
    "\n",
    "    for i, j in zip(remove[0], remove[1]):\n",
    "        word_similarity[i][j] = 0\n",
    "        word_similarity[j][i] = 0\n",
    "\n",
    "    word_similarity = word_similarity > cutoff\n",
    "    word_similarity = word_similarity.astype(int)\n",
    "    np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "    inds = np.where(word_similarity==1)\n",
    "\n",
    "    embeds = {words.index(j):[] for j in l}\n",
    "\n",
    "    for i, j in zip(inds[0], inds[1]):\n",
    "        embeds[words.index(l[i])] += [words.index(l[j])]\n",
    "\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"twitter_airline_2\"\n",
    "\n",
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "cutoffs = [0.3, 0.6]\n",
    "\n",
    "n_cores = 40\n",
    "# n_docs = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ = getDF('datasets/reviews_Electronics_5.json.gz')\n",
    "# dataset_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets_raw/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.airline_sentiment_confidence>=0.5][['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.airline_sentiment = dataset.airline_sentiment.map({'negative':0, 'neutral':1, 'positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns={'text': 'text', 'airline_sentiment': 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14404, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(dataset.shape[0]/n_cores)\n",
    "list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "processed_list_df = pool.map(process_df, list_df)\n",
    "pool.close()\n",
    "\n",
    "dataset = pd.concat(processed_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.text.apply(lambda x: len(x.split(\" \"))>6 and len(x.split(\" \"))<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7168, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"datasets/\"+ dataset_name + \"_\" + str(n_docs) + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOccurenceMatrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "barren = np.where(wordOccurenceMatrix.sum(1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertvocab = words + ['[CLS]', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bertvocab).to_csv(\"resources/bertvocab_\" + dataset_name + \"_\" + str(n_docs)+\".txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1711it [00:00, 8530.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:46, 8539.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_dim = 300\n",
    "glove_embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [00:00<00:00, 18127.80it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_word_embeddings = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    emb = glove_embeddings_index.get(word, np.array([0]*glove_embedding_dim))\n",
    "    glove_word_embeddings.append(emb.tolist())\n",
    "\n",
    "glove_word_embeddings = np.array(glove_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ['glove', glove_word_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 28s, sys: 4.87 s, total: 4min 33s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fasttext_embedding_dim = 300\n",
    "fasttext_embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [00:00<00:00, 11988.40it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_word_embeddings = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    emb = np.array([0]*glove_embedding_dim)\n",
    "    try:\n",
    "        emb = fasttext_embeddings_index[word]\n",
    "    except:\n",
    "        pass\n",
    "    fasttext_word_embeddings.append(emb.tolist())\n",
    "\n",
    "fasttext_word_embeddings = np.array(fasttext_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['fasttext', fasttext_word_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 0.3\n",
      "glove 0.6\n",
      "fasttext 0.3\n",
      "fasttext 0.6\n"
     ]
    }
   ],
   "source": [
    "for embedding_name, word_embeddings in [g, f]:\n",
    "    for cutoff in cutoffs:\n",
    "        print(embedding_name, cutoff)\n",
    "        word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "        remove = np.where(word_similarity == 1)\n",
    "\n",
    "        for i, j in zip(remove[0], remove[1]):\n",
    "            word_similarity[i][j] = 0\n",
    "            word_similarity[j][i] = 0\n",
    "\n",
    "        word_similarity = word_similarity > cutoff\n",
    "        word_similarity = word_similarity.astype(int)\n",
    "        np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "        wordOccuranceMatrixBinary = wordOccurenceMatrix.copy()\n",
    "        wordOccuranceMatrixBinary[wordOccuranceMatrixBinary > 1] = 1\n",
    "\n",
    "        pool = multiprocessing.Pool(n_cores)\n",
    "        similar_words = pool.map(get_edges, wordOccuranceMatrixBinary)\n",
    "        pool.close()\n",
    "        pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "        pickle.dump(similar_words, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embedding & Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file=\"resources/bertvocab_\" + dataset_name + \".txt\", never_split=True, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [tokenizer.tokenize(i) for i in dataset.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_text(text):\n",
    "    return [j for j in text if j in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "temp = pool.map(get_tokenized_text, tokenized_text)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# temp = []\n",
    "# for i in tokenized_text:\n",
    "#     t = [j for j in i if j in words]\n",
    "#     temp.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = keras.preprocessing.sequence.pad_sequences(indexed_tokens, padding='post', dtype='long', maxlen=max([len(i) for i in indexed_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.split(input_ids, 500, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_length = [len(i) for i in indexed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "similar_words_bert = []\n",
    "similar_words_bert_attention = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in tqdm(input_ids):\n",
    "\n",
    "    all_embeddings, _, _, all_attentions = model(batch)\n",
    "    idx_copy = deepcopy(idx)\n",
    "    \n",
    "    print(idx)\n",
    "    for one_embedding in all_embeddings.detach().numpy():\n",
    "        word_embeddings = one_embedding[:pad_length[idx]]\n",
    "        word_similarity = cosine_similarity(word_embeddings)\n",
    "        remove = np.where(word_similarity == 1.000) # to remove self words coupling\n",
    "\n",
    "        for i, j in zip(remove[0], remove[1]):\n",
    "            word_similarity[i][j] = 0\n",
    "            word_similarity[j][i] = 0\n",
    "\n",
    "        word_similarity = word_similarity > cutoff\n",
    "        word_similarity = word_similarity.astype(int)\n",
    "        np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "        inds = np.where(word_similarity==1)\n",
    "        embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "        for i, j in zip(inds[0], inds[1]):\n",
    "            embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "        similar_words_bert.append(embeds)\n",
    "    \n",
    "    idx = deepcopy(idx_copy)\n",
    "    \n",
    "    print(idx)\n",
    "    \n",
    "    for one_attentions in all_attentions[0].detach().numpy():\n",
    "\n",
    "        one_side_edges = np.argmax(one_attentions[9], axis=1) #taking 9 layer of attention\n",
    "        embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "        for j, i in enumerate(one_side_edges[:pad_length[idx]]):\n",
    "            if i < pad_length[idx]:\n",
    "                embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "        similar_words_bert_attention.append(embeds)\n",
    "        idx += 1\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert' + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "pickle.dump(similar_words_bert, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert_attention'+ \".pickle\",\"wb\")\n",
    "pickle.dump(similar_words_bert_attention, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### POS\n",
    "#         pp = np.array([i[1] for i in nltk.pos_tag(words)])\n",
    "#         pp[pp=='JJ'] = 1\n",
    "#         pp[pp=='JJR'] = 1\n",
    "#         pp[pp=='JJS'] = 1\n",
    "#         pp[pp=='NN'] = 1\n",
    "#         pp[pp=='NNS'] = 1\n",
    "#         pp[pp=='NNP'] = 1\n",
    "#         pp[pp=='NNPS'] = 1\n",
    "#         pp[pp!='1'] = 0\n",
    "#         pp = pp.astype(int)\n",
    "\n",
    "#         wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[0].sum()\n",
    "\n",
    "# np.sum(wordOccuranceMatrixBinary)\n",
    "\n",
    "# Counter(np.array([i[1] for i in nltk.pos_tag(words)]))\n",
    "\n",
    "# pp.sum()\n",
    "\n",
    "# np.where(pp!=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for embedding_name in ['bert', 'elmo']:\n",
    "#     for cutoff in cutoffs:\n",
    "#         print(embedding_name, cutoff)\n",
    "#         pool = multiprocessing.Pool(n_cores)\n",
    "#         similar_words = pool.map(get_edges_transformers, dataset.text.tolist())\n",
    "#         pool.close()\n",
    "#         pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) + \"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "#         pickle.dump(similar_words, pickle_out)\n",
    "#         pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pd = pd.apply(lambda x: convert_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_df(df):\n",
    "#     df['text'] = preprocess(df['reviewText'])\n",
    "    \n",
    "# #     pool = multiprocessing.Pool(n_cores)\n",
    "# #     df['cleaned'] = pool.map(process_l, df['text'].tolist())\n",
    "# #     pool.close()\n",
    "    \n",
    "# #     df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
