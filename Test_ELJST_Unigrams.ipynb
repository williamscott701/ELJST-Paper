{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from nltk.stem import PorterStemmer\n",
    "import joblib \n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imp\n",
    "import copy\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as my_utils\n",
    "import ELJST_script_unigram as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "maxIters = 20\n",
    "\n",
    "beta = .01\n",
    "gamma = 10\n",
    "n_topics = 5\n",
    "n_sentiment = 5\n",
    "lambda_param = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"amazon_movies_20000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/\"+ dataset_name + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentiment = dataset.sentiment.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.94365"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = \"fasttext_0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = pickle.load(open(\"resources/\"+ dataset_name + \"_\" +embedding_name + \".pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 578 ms, sys: 4.16 ms, total: 582 ms\n",
      "Wall time: 581 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in similar_words:\n",
    "    for i in s.keys():\n",
    "        k = list(set(s[i]))\n",
    "        if i in k:\n",
    "            k.remove(i)\n",
    "        s[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_name = \"glove_0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_words = [{} for i in range(dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1/n_topics * np.ones(n_topics)\n",
    "gamma = [gamma/(n_topics*n_sentiment)]*n_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ELJST_script_unigram' from '/home/william18026/ELJST-Paper/ELJST_script_unigram.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 108/20000 [00:30<44:31,  7.45it/s]"
     ]
    }
   ],
   "source": [
    "sampler = lda.SentimentLDAGibbsSampler(n_topics, alpha, beta, gamma, numSentiments=n_sentiment, \n",
    "                                       SentimentRange = n_sentiment, max_df = max_df, min_df = min_df, \n",
    "                                       lambda_param = lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5863 (20000, 5863)\n",
      "CPU times: user 21.9 s, sys: 1.01 s, total: 22.9 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampler._initialize_(reviews = dataset.text.tolist(), labels = dataset.sentiment.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 3/20000 [00:00<12:00, 27.75it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** fasttext_0.3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 8/20000 [00:00<10:52, 30.64it/s]\u001b[A\n",
      "  0%|          | 12/20000 [00:00<10:37, 31.37it/s]\u001b[A\n",
      "  0%|          | 17/20000 [00:00<09:26, 35.30it/s]\u001b[A\n",
      "  0%|          | 21/20000 [00:00<09:18, 35.78it/s]\u001b[A\n",
      "  0%|          | 25/20000 [00:00<10:07, 32.86it/s]\u001b[A\n",
      "  0%|          | 29/20000 [00:00<10:00, 33.25it/s]\u001b[A\n",
      "  0%|          | 34/20000 [00:00<09:13, 36.10it/s]\u001b[A\n",
      "  0%|          | 38/20000 [00:01<10:58, 30.30it/s]\u001b[A\n",
      "  0%|          | 44/20000 [00:01<09:29, 35.06it/s]\u001b[A\n",
      "  0%|          | 49/20000 [00:01<08:38, 38.50it/s]\u001b[A\n",
      "  0%|          | 56/20000 [00:01<07:30, 44.28it/s]\u001b[A\n",
      "  0%|          | 63/20000 [00:01<06:48, 48.79it/s]\u001b[A\n",
      "  0%|          | 69/20000 [00:01<06:52, 48.34it/s]\u001b[A\n",
      "  0%|          | 78/20000 [00:01<06:05, 54.52it/s]\u001b[A\n",
      "  0%|          | 84/20000 [00:01<07:26, 44.57it/s]\u001b[A\n",
      "  0%|          | 90/20000 [00:02<08:30, 38.98it/s]\u001b[A\n",
      "  0%|          | 95/20000 [00:02<09:50, 33.68it/s]\u001b[A\n",
      "  0%|          | 99/20000 [00:02<09:35, 34.59it/s]\u001b[A\n",
      "  1%|          | 103/20000 [00:02<10:11, 32.54it/s]\u001b[A\n",
      "  1%|          | 108/20000 [00:02<09:20, 35.46it/s]\u001b[A\n",
      "  1%|          | 114/20000 [00:02<08:46, 37.75it/s]\u001b[A\n",
      "  1%|          | 120/20000 [00:02<08:09, 40.63it/s]\u001b[A\n",
      "  1%|          | 125/20000 [00:03<08:06, 40.86it/s]\u001b[A\n",
      "  1%|          | 131/20000 [00:03<07:35, 43.58it/s]\u001b[A\n",
      "  1%|          | 136/20000 [00:03<07:58, 41.52it/s]\u001b[A\n",
      "  1%|          | 141/20000 [00:03<09:10, 36.05it/s]\u001b[A\n",
      "  1%|          | 145/20000 [00:03<09:17, 35.59it/s]\u001b[A\n",
      "  1%|          | 149/20000 [00:03<10:21, 31.96it/s]\u001b[A\n",
      "  1%|          | 154/20000 [00:03<09:35, 34.46it/s]\u001b[A\n",
      "  1%|          | 159/20000 [00:04<09:03, 36.54it/s]\u001b[A\n",
      "  1%|          | 165/20000 [00:04<08:13, 40.21it/s]\u001b[A\n",
      "  1%|          | 170/20000 [00:04<08:24, 39.27it/s]\u001b[A\n",
      "  1%|          | 177/20000 [00:04<07:33, 43.75it/s]\u001b[A\n",
      "  1%|          | 182/20000 [00:04<07:24, 44.55it/s]\u001b[A\n",
      "  1%|          | 190/20000 [00:04<06:48, 48.51it/s]\u001b[A\n",
      "  1%|          | 196/20000 [00:04<07:07, 46.37it/s]\u001b[A\n",
      "  1%|          | 201/20000 [00:04<08:17, 39.78it/s]\u001b[A\n",
      "  1%|          | 206/20000 [00:05<09:08, 36.09it/s]\u001b[A\n",
      "  1%|          | 210/20000 [00:05<09:45, 33.77it/s]\u001b[A\n",
      "  1%|          | 214/20000 [00:05<09:45, 33.78it/s]\u001b[A\n",
      "  1%|          | 218/20000 [00:05<09:42, 33.97it/s]\u001b[A\n",
      "  1%|          | 222/20000 [00:05<09:21, 35.24it/s]\u001b[A\n",
      "  1%|          | 230/20000 [00:05<07:57, 41.40it/s]\u001b[A\n",
      "  1%|          | 237/20000 [00:05<07:00, 47.05it/s]\u001b[A\n",
      "  1%|          | 243/20000 [00:05<07:33, 43.56it/s]\u001b[A\n",
      "  1%|          | 248/20000 [00:06<07:33, 43.57it/s]\u001b[A\n",
      "  1%|▏         | 253/20000 [00:06<07:59, 41.21it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3d4e6d74160e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m sampler.run(name=embedding_name, reviews=dataset.text.tolist(), labels=dataset.sentiment.tolist(), \n\u001b[0;32m----> 2\u001b[0;31m             similar_words=similar_words, mrf=True, maxIters=20, debug=True)\n\u001b[0m",
      "\u001b[0;32m~/ELJST-Paper/ELJST_script_unigram.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, name, reviews, labels, similar_words, unlabeled_reviews, mrf, maxIters, debug)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m#                     print(d, v, idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                     \u001b[0mprobabilities_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditionalDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m                     \u001b[0;31m#if v in self.priorSentiment:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                     \u001b[0;31m#    s = self.priorSentiment[v]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ELJST-Paper/ELJST_script_unigram.py\u001b[0m in \u001b[0;36mconditionalDistribution\u001b[0;34m(self, d, v, similar_words, mrf, debug_mode)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# all_children = similar_words[v,:] #[d][i1,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mnew_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_children\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mtopic_assignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m#             topic_assignment /= topic_assignment.sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler.run(name=embedding_name, reviews=dataset.text.tolist(), labels=dataset.sentiment.tolist(), \n",
    "            similar_words=similar_words, mrf=True, maxIters=20, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalDistribution_new(sampler, d, v, similar_words, mrf = True, debug_mode=False):\n",
    "        \"\"\"\n",
    "        Calculates the (topic, sentiment) probability for word v in document d\n",
    "        Returns:    a matrix (numTopics x numSentiments) storing the probabilities\n",
    "        \"\"\"\n",
    "        probabilities_ts = np.ones((sampler.numTopics, sampler.numSentiments))\n",
    "        topic_assignment1 = np.ones((sampler.numTopics, sampler.numSentiments))\n",
    "        \n",
    "        firstFactor = (sampler.n_dt[d] + sampler.alphaVec) / \\\n",
    "            (sampler.n_d[d] + np.sum(sampler.alphaVec))\n",
    "        \n",
    "        secondFactor = np.zeros((sampler.numTopics,sampler.numSentiments))\n",
    "        for k in range(sampler.numTopics):\n",
    "            secondFactor[k,:] = (sampler.n_dts[d, k, :] + sampler.sentimentprior[d]) / \\\n",
    "                (sampler.n_dt[d, k] + np.sum(sampler.sentimentprior[d]))\n",
    "        thirdFactor = (sampler.n_vts[v, :, :] + sampler.beta) / \\\n",
    "            (sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta)\n",
    "        \n",
    "        probabilities_ts *= firstFactor[:, np.newaxis]\n",
    "        probabilities_ts *= secondFactor * thirdFactor\n",
    "        #probabilities_ts = np.exp(probabilities_ts)\n",
    "        probabilities_ts /= np.sum(probabilities_ts)\n",
    "        \n",
    "        if mrf == True and sampler.lambda_param != 0:\n",
    "            all_children = np.zeros(sampler.wordOccuranceMatrix_shape).astype(int)\n",
    "            try:\n",
    "                all_children[similar_words[v]] = 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # all_children = similar_words[v,:] #[d][i1,:]\n",
    "            new_C = sampler.vts[all_children,:, :]\n",
    "            topic_assignment = new_C.sum(0)\n",
    "            #topic_assignment = np.exp(topic_assignment)\n",
    "            topic_assignment /= topic_assignment.sum()\n",
    "            topic_assignment1 = np.exp(sampler.lambda_param * topic_assignment)\n",
    "            topic_assignment1 /= topic_assignment1.sum()\n",
    "            \n",
    "        if debug_mode == True:\n",
    "            print (probabilities_ts, topic_assignment1)\n",
    "        \n",
    "        probabilities_ts *= topic_assignment1\n",
    "        probabilities_ts /= np.sum(probabilities_ts)\n",
    "\n",
    "        if debug_mode == True:\n",
    "            print (probabilities_ts)\n",
    "            \n",
    "        return probabilities_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sampler.n_vts[0,:,:] + sampler.beta)/(sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 7054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[d].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.wordOccuranceMatrix[d].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (conditionalDistribution_new(sampler,d,0,similar_words,debug_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sampler, \"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = joblib.load(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.loglikelihood_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),\n",
    "                 sampler.dt_distribution.argmax(axis=1), metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix, sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, list(sampler.getTopKWords(5).values()), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_utils.get_hscore_multi(sampler.dt_distribution, sampler.wordOccuranceMatrix, n_topics, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
