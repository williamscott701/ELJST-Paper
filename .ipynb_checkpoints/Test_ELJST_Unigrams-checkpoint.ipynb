{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from nltk.stem import PorterStemmer\n",
    "import joblib \n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imp\n",
    "import copy\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as my_utils\n",
    "import ELJST_script_unigram as lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "maxIters = 20\n",
    "\n",
    "beta = .01\n",
    "gamma = 10\n",
    "n_topics = 5\n",
    "n_sentiment = 5\n",
    "lambda_param = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"twitter_airline_2_7168\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/\"+ dataset_name + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentiment = dataset.sentiment.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.710379464285714"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = \"fasttext_0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = pickle.load(open(\"resources/\"+ dataset_name + \"_\" +embedding_name + \".pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.3 ms, sys: 236 Âµs, total: 56.6 ms\n",
      "Wall time: 56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in similar_words:\n",
    "    for i in s.keys():\n",
    "        k = list(set(s[i]))\n",
    "        if i in k:\n",
    "            k.remove(i)\n",
    "        s[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_name = \"glove_0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar_words = [{} for i in range(dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1/n_topics * np.ones(n_topics)\n",
    "gamma = [gamma/(n_topics*n_sentiment)]*n_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ELJST_script_unigram' from '/home/william18026/Embedding-LJST-Paper/ELJST_script_unigram.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.SentimentLDAGibbsSampler(n_topics, alpha, beta, gamma, numSentiments=n_sentiment, \n",
    "                                       SentimentRange = n_sentiment, max_df = max_df, min_df = min_df, \n",
    "                                       lambda_param = lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283 (7168, 1283)\n",
      "CPU times: user 4.25 s, sys: 103 ms, total: 4.36 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampler._initialize_(reviews = dataset.text.tolist(), labels = dataset.sentiment.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fasttext_0.3 0\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william18026/Embedding-LJST-Paper/ELJST_script_unigram.py:226: RuntimeWarning: overflow encountered in exp\n",
      "  topic_assignment1 = np.exp(self.lambda_param * topic_assignment)\n",
      "/home/william18026/Embedding-LJST-Paper/ELJST_script_unigram.py:227: RuntimeWarning: invalid value encountered in true_divide\n",
      "  topic_assignment1 /= topic_assignment1.sum()\n",
      "/home/william18026/Embedding-LJST-Paper/ELJST_script_unigram.py:59: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return np.random.multinomial(1, theta).argmax()\n",
      "/home/william18026/Embedding-LJST-Paper/ELJST_script_unigram.py:62: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return np.random.multinomial(1, theta).argmax()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pvals < 0, pvals > 1 or pvals contains NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Embedding-LJST-Paper/ELJST_script_unigram.py\u001b[0m in \u001b[0;36msampleFromCategorical\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pvals < 0, pvals > 1 or pvals contains NaNs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3d4e6d74160e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m sampler.run(name=embedding_name, reviews=dataset.text.tolist(), labels=dataset.sentiment.tolist(), \n\u001b[0;32m----> 2\u001b[0;31m             similar_words=similar_words, mrf=True, maxIters=20, debug=True)\n\u001b[0m",
      "\u001b[0;32m~/Embedding-LJST-Paper/ELJST_script_unigram.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, name, reviews, labels, similar_words, unlabeled_reviews, mrf, maxIters, debug)\u001b[0m\n\u001b[1;32m    359\u001b[0m                     \u001b[0;31m#    t = sampleFromCategorical(probabilities_ts[:, s])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampleFromCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding-LJST-Paper/ELJST_script_unigram.py\u001b[0m in \u001b[0;36msampleFromCategorical\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordOccuranceVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pvals < 0, pvals > 1 or pvals contains NaNs"
     ]
    }
   ],
   "source": [
    "sampler.run(name=embedding_name, reviews=dataset.text.tolist(), labels=dataset.sentiment.tolist(), \n",
    "            similar_words=similar_words, mrf=True, maxIters=20, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalDistribution_new(sampler, d, v, similar_words, mrf = True, debug_mode=False):\n",
    "        \"\"\"\n",
    "        Calculates the (topic, sentiment) probability for word v in document d\n",
    "        Returns:    a matrix (numTopics x numSentiments) storing the probabilities\n",
    "        \"\"\"\n",
    "        probabilities_ts = np.ones((sampler.numTopics, sampler.numSentiments))\n",
    "        topic_assignment1 = np.ones((sampler.numTopics, sampler.numSentiments))\n",
    "        \n",
    "        firstFactor = (sampler.n_dt[d] + sampler.alphaVec) / \\\n",
    "            (sampler.n_d[d] + np.sum(sampler.alphaVec))\n",
    "        \n",
    "        secondFactor = np.zeros((sampler.numTopics,sampler.numSentiments))\n",
    "        for k in range(sampler.numTopics):\n",
    "            secondFactor[k,:] = (sampler.n_dts[d, k, :] + sampler.sentimentprior[d]) / \\\n",
    "                (sampler.n_dt[d, k] + np.sum(sampler.sentimentprior[d]))\n",
    "        thirdFactor = (sampler.n_vts[v, :, :] + sampler.beta) / \\\n",
    "            (sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta)\n",
    "        \n",
    "        probabilities_ts *= firstFactor[:, np.newaxis]\n",
    "        probabilities_ts *= secondFactor * thirdFactor\n",
    "        #probabilities_ts = np.exp(probabilities_ts)\n",
    "        probabilities_ts /= np.sum(probabilities_ts)\n",
    "        \n",
    "        if mrf == True and sampler.lambda_param != 0:\n",
    "            all_children = np.zeros(sampler.wordOccuranceMatrix_shape).astype(int)\n",
    "            try:\n",
    "                all_children[similar_words[v]] = 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # all_children = similar_words[v,:] #[d][i1,:]\n",
    "            new_C = sampler.vts[all_children,:, :]\n",
    "            topic_assignment = new_C.sum(0)\n",
    "            #topic_assignment = np.exp(topic_assignment)\n",
    "            topic_assignment /= topic_assignment.sum()\n",
    "            topic_assignment1 = np.exp(sampler.lambda_param * topic_assignment)\n",
    "            topic_assignment1 /= topic_assignment1.sum()\n",
    "            \n",
    "        if debug_mode == True:\n",
    "            print (probabilities_ts, topic_assignment1)\n",
    "        \n",
    "        probabilities_ts *= topic_assignment1\n",
    "        probabilities_ts /= np.sum(probabilities_ts)\n",
    "\n",
    "        if debug_mode == True:\n",
    "            print (probabilities_ts)\n",
    "            \n",
    "        return probabilities_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6981.83, 2148.83, 1306.83],\n",
       "       [7050.83, 2146.83, 1293.83],\n",
       "       [6985.83, 2073.83, 1286.83],\n",
       "       [6970.83, 1965.83, 1305.83],\n",
       "       [7016.83, 2163.83, 1150.83]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.31119062e-04, 4.65369527e-06, 7.65210471e-06],\n",
       "       [1.43245547e-04, 4.65803068e-06, 7.72899067e-06],\n",
       "       [2.87725295e-04, 4.82199602e-06, 7.77103425e-06],\n",
       "       [2.88344430e-04, 5.08690985e-06, 7.65796467e-06],\n",
       "       [1.43939642e-04, 4.62143514e-06, 8.68938071e-06]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sampler.n_vts[0,:,:] + sampler.beta)/(sampler.n_ts + sampler.n_vts.shape[0] * sampler.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 7054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'americanair tomorrow fail flight today available seat available'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[d].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordOccuranceMatrix[d].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.91299230e-01 3.35703053e-03 3.44999346e-04]\n",
      " [8.21380335e-04 4.45158084e-06 7.38643199e-06]\n",
      " [1.64983767e-03 4.60827903e-06 7.42661214e-06]\n",
      " [1.65338783e-03 4.86145155e-06 7.31855395e-06]\n",
      " [8.25360327e-04 4.41660727e-06 8.30425632e-06]] [[0.07583049 0.06208475 0.06208475]\n",
      " [0.07583049 0.06208475 0.06208475]\n",
      " [0.07583049 0.06208475 0.06208475]\n",
      " [0.07583049 0.06208475 0.06208475]\n",
      " [0.07583049 0.06208475 0.06208475]]\n",
      "[[9.91973678e-01 2.75037413e-03 2.82653752e-04]\n",
      " [8.21939175e-04 3.64712584e-06 6.05161355e-06]\n",
      " [1.65096017e-03 3.77550675e-06 6.08453266e-06]\n",
      " [1.65451274e-03 3.98292790e-06 5.99600190e-06]\n",
      " [8.25921876e-04 3.61847241e-06 6.80357582e-06]]\n",
      "[[9.91973678e-01 2.75037413e-03 2.82653752e-04]\n",
      " [8.21939175e-04 3.64712584e-06 6.05161355e-06]\n",
      " [1.65096017e-03 3.77550675e-06 6.08453266e-06]\n",
      " [1.65451274e-03 3.98292790e-06 5.99600190e-06]\n",
      " [8.25921876e-04 3.61847241e-06 6.80357582e-06]]\n"
     ]
    }
   ],
   "source": [
    "print (conditionalDistribution_new(sampler,d,0,similar_words,debug_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-328530.5444470495"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sampler, \"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = joblib.load(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.loglikelihood_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(euclidean_distances(sampler.wordOccuranceMatrix),\n",
    "                 sampler.dt_distribution.argmax(axis=1), metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_bouldin_score(sampler.wordOccuranceMatrix, sampler.dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_utils.coherence_score(sampler.wordOccuranceMatrix, list(sampler.getTopKWords(5).values()), sampler.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_utils.get_hscore_multi(sampler.dt_distribution, sampler.wordOccuranceMatrix, n_topics, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_new",
   "language": "python",
   "name": "python3_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
