{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os, glob\n",
    "import string\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import joblib, multiprocessing\n",
    "import utils as my_utils\n",
    "\n",
    "from scipy import spatial\n",
    "from collections import Counter\n",
    "from scipy.special import gammaln\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "\n",
    "n_cores = -1\n",
    "max_iter = 20\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = glob.glob(\"datasets/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['datasets/amazon_home_20000_dataset',\n",
    "            'datasets/amazon_movies_20000_dataset',\n",
    "            'datasets/amazon_kindle_20000_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21299910113664436 -0.014870528600495302 13.555274042059997 -96.63780695289674 -2120719.0229813205 1227.647341805287 0.10671899911990827\n",
      "0.4203674904005266 -0.07748208423443771 8.780615084283017 -118.66992959639146 -2172351.57338223 1459.76328302013 0.5078804534905659\n",
      "0.5202291400991184 -0.07379117504853107 7.761375132675162 -131.75379881010284 -2233032.4308632123 1789.2505285955313 0.2649350973954502\n",
      "0.6136914707530757 -0.06543443611692272 6.932836519638886 -145.33511949007286 -2341748.088304888 2576.4880891669527 -0.5389361216754098\n",
      "0.27610960562394077 -0.016196728494500462 13.622293486251337 -95.6622764235246 -2192092.5131938914 1802.8556873036562 0.1944755527200007\n",
      "0.4468566998022109 -0.07780590497186958 11.100222830922561 -117.7057578342366 -2260124.306228386 2275.1498837290605 0.3196616283736684\n",
      "0.5085470490697575 -0.08185449369760278 10.879818425541634 -139.2363511627342 -2336124.5904728733 2950.4946269267784 -0.16816276104459704\n",
      "0.5587034512300336 -0.07658492129903088 9.695449941550782 -149.58321687360484 -2438184.6748951315 4182.98985265147 -1.048247081320312\n",
      "0.2313044301674228 -0.019039066830866896 12.663156131006824 -88.00248337604843 -1956521.554685196 1228.3902047225506 0.23642373545975837\n",
      "0.43591030046463936 -0.06483843074923692 10.612459707345126 -114.48246960078372 -2029682.155974922 1602.715790212315 0.3520778087643626\n",
      "0.5360701302052088 -0.07924599945364554 10.089288487427472 -129.57808573819804 -2099675.101123712 2067.164203186924 0.039137909456832605\n",
      "0.5795460302723497 -0.07894825539164549 8.987488761074564 -145.63823418221713 -2193057.7087147054 2902.856577743676 -1.0890708583861244\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    for n_topics in [5, 25, 50, 100]:\n",
    "#         print(d, n_topics)\n",
    "\n",
    "        dataset = pd.read_pickle(d)\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                                     stop_words=\"english\", max_features=max_features,\n",
    "                                     max_df=max_df, min_df=min_df)\n",
    "\n",
    "        count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "        words = vectorizer.get_feature_names()\n",
    "\n",
    "        vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "\n",
    "        model = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, n_jobs=n_cores, verbose=0)\n",
    "\n",
    "        dt_distribution = model.fit_transform(count_matrix)\n",
    "\n",
    "        topic_words = {}\n",
    "        for topic, comp in enumerate(model.components_):\n",
    "            word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "            topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "        sample_df = []\n",
    "        for topic, word in topic_words.items():\n",
    "            sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "\n",
    "        print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "              silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "              davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "              my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "              model.score(count_matrix),\n",
    "              model.perplexity(count_matrix),\n",
    "              my_utils.coherence_score2(count_matrix, sample_df, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/amazon_electronics_20000_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(zip(words,np.arange(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=n_topics, max_iter=20, n_jobs=n_cores, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=20,\n",
       "                          mean_change_tol=0.001, n_components=100, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = {}\n",
    "for topic, comp in enumerate(model.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "sample_df = []\n",
    "for topic, word in topic_words.items():\n",
    "    sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "dt_distribution = model.transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william18026/ELJST-Paper/utils.py:230: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intradist += tmp.sum()*1.0/(cnt*(cnt-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan -0.0704372858685484 6.893335066044154 -144.7036173502089 -2358281.0861068484 2407.808612401552\n"
     ]
    }
   ],
   "source": [
    "print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "      silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "      model.score(count_matrix),\n",
    "      model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000))\n",
    "# print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "# print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "# print(\"Perplexity:\", model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, k, 3000))\n",
    "#     print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "#     print(\"Perplexity:\", model.perplexity(count_matrix))\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "    \n",
    "# #     print my_utils.coherence_score(count_matrix, sample_df, vocabulary), \"\\t\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)), \"\\t\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_evaluations_multi(model):\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     h_score =  my_utils.get_hscore_multi(dt_distribution, count_matrix, k)\n",
    "#     likelihood =  model.score(count_matrix)\n",
    "#     perplexity = model.perplexity(count_matrix)\n",
    "#     coherance_score = my_utils.coherence_score(count_matrix, sample_df, vocabulary)\n",
    "#     silhouette = silhouette_score(count_matrix, dt_distribution.argmax(axis=1))\n",
    "#     return [h_score, likelihood, perplexity, coherance_score, silhouette]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"H-Score:\", my_utils.get_hscore(dt_distribution, count_matrix, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(dt_distribution)\n",
    "\n",
    "# X_embedded.shape\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter([i[0] for i in X_embedded], [i[1] for i in X_embedded], c=dt_distribution.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
