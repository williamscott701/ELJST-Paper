{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_embedding import BertEmbedding\n",
    "# from allennlp.commands.elmo import ElmoEmbedder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import *\n",
    "#import torch\n",
    "import keras\n",
    "\n",
    "import imp, gzip\n",
    "import pickle, nltk\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as my_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(i):\n",
    "    t = np.where(i>0)[0]\n",
    "    comb = combinations(t, 2)\n",
    "    embeds = {j:[] for j in t}\n",
    "\n",
    "    for p, q in comb:\n",
    "        if word_similarity[p][q]:\n",
    "            embeds[p] += [q]\n",
    "            embeds[q] += [p]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in tqdm(parse(path)):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df['text'] = my_utils.preprocess(df['text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_transformers(text):\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    if embedding_name == 'bert':\n",
    "        results = bert_embedding(sentence)\n",
    "        embed_vecs = np.array([i[1][0] for i in results])\n",
    "    else:\n",
    "        embed_vecs = elmo.embed_sentence(sentence)[2]\n",
    "\n",
    "    l = np.array(list(set(sentence).intersection(words)))\n",
    "\n",
    "    pp = np.array([i[1] for i in nltk.pos_tag(l)])\n",
    "    pp[pp=='JJ'] = 1\n",
    "    pp[pp=='JJR'] = 1\n",
    "    pp[pp=='JJS'] = 1\n",
    "    pp[pp=='NN'] = 1\n",
    "    pp[pp=='NNS'] = 1\n",
    "    pp[pp=='NNP'] = 1\n",
    "    pp[pp=='NNPS'] = 1\n",
    "    pp[pp!='1'] = 0\n",
    "    pp = pp.astype(int)\n",
    "\n",
    "    l = l[pp==1]\n",
    "\n",
    "    word_embeddings = np.array([embed_vecs[sentence.index(i)] for i in l])\n",
    "\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    remove = np.where(word_similarity == 1)\n",
    "\n",
    "    for i, j in zip(remove[0], remove[1]):\n",
    "        word_similarity[i][j] = 0\n",
    "        word_similarity[j][i] = 0\n",
    "\n",
    "    word_similarity = word_similarity > cutoff\n",
    "    word_similarity = word_similarity.astype(int)\n",
    "    np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "    inds = np.where(word_similarity==1)\n",
    "\n",
    "    embeds = {words.index(j):[] for j in l}\n",
    "\n",
    "    for i, j in zip(inds[0], inds[1]):\n",
    "        embeds[words.index(l[i])] += [words.index(l[j])]\n",
    "\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"twitter_apple\"\n",
    "\n",
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "cutoffs = [0.3, 0.6]\n",
    "\n",
    "n_cores = 40\n",
    "n_docs = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3886, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = pd.read_csv('nongit_resources/Apple-Twitter-Sentiment-DFE.csv', encoding = \"ISO-8859-1\")\n",
    "dataset_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'3': 2162, '5': 423, '1': 1219, 'not_relevant': 82})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dataset_.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dataset = train_test_split(dataset_, test_size=0.99, random_state=77, stratify=dataset_.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3848, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3848, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(dataset.shape[0]/n_cores)\n",
    "list_df = [dataset[i:i+n] for i in range(0, dataset.shape[0],n)]\n",
    "\n",
    "pool = multiprocessing.Pool(n_cores)\n",
    "processed_list_df = pool.map(process_df, list_df)\n",
    "pool.close()\n",
    "\n",
    "dataset = pd.concat(processed_list_df)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.909823284823285"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3848, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"datasets/\"+ dataset_name + \"_\" + str(n_docs) + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/\"+ dataset_name + \"_\" + str(n_docs) + \"_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOccurenceMatrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "barren = np.where(wordOccurenceMatrix.sum(1)<=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    6,   14,   28,   29,   47,   49,   53,   57,   63,   67,\n",
       "         77,   79,   86,   89,   98,  100,  120,  148,  149,  157,  161,\n",
       "        167,  177,  189,  223,  249,  255,  256,  270,  275,  278,  286,\n",
       "        298,  303,  312,  316,  323,  331,  334,  347,  354,  364,  377,\n",
       "        380,  382,  389,  401,  406,  408,  419,  428,  440,  441,  442,\n",
       "        450,  458,  459,  470,  477,  481,  482,  484,  488,  494,  499,\n",
       "        503,  507,  513,  520,  522,  531,  534,  535,  559,  563,  569,\n",
       "        572,  583,  590,  591,  597,  612,  627,  634,  641,  643,  646,\n",
       "        660,  680,  681,  687,  689,  690,  701,  706,  707,  708,  713,\n",
       "        719,  723,  729,  737,  743,  762,  790,  792,  803,  806,  811,\n",
       "        813,  831,  834,  847,  853,  877,  887,  930,  937,  939,  944,\n",
       "        948,  954,  969,  972,  978,  979,  988,  992, 1003, 1011, 1019,\n",
       "       1030, 1032, 1039, 1052, 1056, 1064, 1077, 1080, 1082, 1085, 1088,\n",
       "       1099, 1100, 1101, 1119, 1122, 1123, 1124, 1126, 1144, 1147, 1150,\n",
       "       1159, 1162, 1166, 1172, 1174, 1176, 1177, 1181, 1189, 1197, 1204,\n",
       "       1206, 1208, 1212, 1214, 1220, 1233, 1237, 1253, 1256, 1261, 1271,\n",
       "       1277, 1285, 1303, 1309, 1312, 1316, 1332, 1346, 1349, 1350, 1352,\n",
       "       1358, 1371, 1409, 1410, 1416, 1419, 1421, 1422, 1423, 1424, 1449,\n",
       "       1455, 1477, 1496, 1509, 1522, 1536, 1541, 1551, 1571, 1592, 1617,\n",
       "       1618, 1629, 1632, 1638, 1641, 1642, 1655, 1667, 1669, 1675, 1686,\n",
       "       1691, 1697, 1720, 1725, 1726, 1730, 1768, 1770, 1777, 1789, 1790,\n",
       "       1794, 1796, 1797, 1801, 1805, 1814, 1831, 1833, 1840, 1842, 1846,\n",
       "       1848, 1849, 1858, 1865, 1874, 1875, 1878, 1881, 1916, 1925, 1927,\n",
       "       1929, 1940, 1941, 1947, 1950, 1951, 1957, 1977, 1983, 1987, 1989,\n",
       "       1996, 2011, 2015, 2020, 2033, 2044, 2047, 2060, 2072, 2076, 2081,\n",
       "       2086, 2093, 2094, 2095, 2105, 2121, 2134, 2140, 2144, 2150, 2156,\n",
       "       2157, 2167, 2170, 2180, 2187, 2197, 2199, 2202, 2204, 2207, 2212,\n",
       "       2224, 2227, 2228, 2232, 2237, 2260, 2271, 2288, 2289, 2291, 2303,\n",
       "       2313, 2319, 2324, 2334, 2340, 2346, 2357, 2360, 2365, 2370, 2378,\n",
       "       2384, 2402, 2403, 2406, 2410, 2420, 2424, 2427, 2429, 2437, 2441,\n",
       "       2446, 2457, 2471, 2493, 2499, 2506, 2513, 2514, 2522, 2523, 2524,\n",
       "       2525, 2534, 2537, 2538, 2542, 2548, 2553, 2559, 2571, 2586, 2594,\n",
       "       2596, 2601, 2608, 2612, 2627, 2629, 2633, 2634, 2640, 2645, 2654,\n",
       "       2659, 2660, 2665, 2666, 2682, 2685, 2687, 2692, 2700, 2702, 2703,\n",
       "       2727, 2735, 2744, 2756, 2761, 2767, 2779, 2781, 2784, 2789, 2799,\n",
       "       2801, 2804, 2805, 2806, 2813, 2820, 2823, 2828, 2829, 2849, 2851,\n",
       "       2906, 2907, 2918, 2939, 2950, 2955, 2956, 2958, 2965, 2967, 2978,\n",
       "       2986, 3001, 3004, 3007, 3022, 3023, 3032, 3063, 3078, 3102, 3118,\n",
       "       3120, 3128, 3132, 3137, 3144, 3153, 3166, 3179, 3184, 3194, 3208,\n",
       "       3214, 3215, 3216, 3218, 3221, 3228, 3237, 3254, 3255, 3261, 3263,\n",
       "       3272, 3282, 3296, 3301, 3313, 3325, 3326, 3327, 3334, 3339, 3343,\n",
       "       3352, 3355, 3358, 3359, 3363, 3365, 3375, 3394, 3399, 3401, 3408,\n",
       "       3411, 3423, 3424, 3425, 3440, 3442, 3451, 3457, 3461, 3479, 3486,\n",
       "       3493, 3502, 3503, 3526, 3534, 3538, 3539, 3545, 3550, 3559, 3564,\n",
       "       3568, 3583, 3587, 3591, 3603, 3611, 3613, 3624, 3650, 3656, 3657,\n",
       "       3663, 3671, 3675, 3677, 3680, 3686, 3687, 3692, 3693, 3695, 3697,\n",
       "       3698, 3700, 3724, 3730, 3744, 3748, 3754, 3761, 3763, 3767, 3775,\n",
       "       3776, 3779, 3780, 3797, 3809, 3813, 3816, 3821, 3830, 3831, 3833,\n",
       "       3835, 3838, 3840, 3847])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertvocab = words + ['[CLS]', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bertvocab).to_csv(\"resources/bertvocab_\" + dataset_name + \"_\" + str(n_docs) + \".txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "856it [00:00, 8553.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:43, 9092.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_dim = 300\n",
    "glove_embeddings_index = loadGloveModel(\"nongit_resources/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 19046.91it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_word_embeddings = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    emb = glove_embeddings_index.get(word, np.array([0]*glove_embedding_dim))\n",
    "    glove_word_embeddings.append(emb.tolist())\n",
    "\n",
    "glove_word_embeddings = np.array(glove_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ['glove', glove_word_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 7s, sys: 5.35 s, total: 4min 12s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fasttext_embedding_dim = 300\n",
    "fasttext_embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\"nongit_resources/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 19243.59it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_word_embeddings = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    emb = np.array([0]*glove_embedding_dim)\n",
    "    try:\n",
    "        emb = fasttext_embeddings_index[word]\n",
    "    except:\n",
    "        pass\n",
    "    fasttext_word_embeddings.append(emb.tolist())\n",
    "\n",
    "fasttext_word_embeddings = np.array(fasttext_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['fasttext', fasttext_word_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 0.3\n"
     ]
    }
   ],
   "source": [
    "for embedding_name, word_embeddings in [g, f]:\n",
    "    for cutoff in cutoffs:\n",
    "        print(embedding_name, cutoff)\n",
    "        word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "        remove = np.where(word_similarity == 1)\n",
    "\n",
    "        for i, j in zip(remove[0], remove[1]):\n",
    "            word_similarity[i][j] = 0\n",
    "            word_similarity[j][i] = 0\n",
    "\n",
    "        word_similarity = word_similarity > cutoff\n",
    "        word_similarity = word_similarity.astype(int)\n",
    "        np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "        wordOccuranceMatrixBinary = wordOccurenceMatrix.copy()\n",
    "        wordOccuranceMatrixBinary[wordOccuranceMatrixBinary > 1] = 1\n",
    "\n",
    "        pool = multiprocessing.Pool(n_cores)\n",
    "        similar_words = pool.map(get_edges, wordOccuranceMatrixBinary)\n",
    "        pool.close()\n",
    "        pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "        pickle.dump(similar_words, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embedding & Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_name = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_weights = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer(vocab_file='bertvocab.txt', never_split=True, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text = [tokenizer.tokenize(i) for i in dataset.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# temp = []\n",
    "# for i in tokenized_text:\n",
    "#     t = [j for j in i if j in words]\n",
    "#     temp.append(t)\n",
    "    \n",
    "# tokenized_text = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexed_tokens = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_ids = keras.preprocessing.sequence.pad_sequences(indexed_tokens, padding='post', dtype='long', maxlen=max([len(i) for i in indexed_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.split(input_ids, 500, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad_length = [len(i) for i in indexed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# similar_words_bert = []\n",
    "# similar_words_bert_attention = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch in tqdm(input_ids):\n",
    "\n",
    "#     all_embeddings, _, _, all_attentions = model(batch)\n",
    "#     idx_copy = deepcopy(idx)\n",
    "    \n",
    "#     print(idx)\n",
    "#     for one_embedding in all_embeddings.detach().numpy():\n",
    "#         word_embeddings = one_embedding[:pad_length[idx]]\n",
    "#         word_similarity = cosine_similarity(word_embeddings)\n",
    "#         remove = np.where(word_similarity == 1.000) # to remove self words coupling\n",
    "\n",
    "#         for i, j in zip(remove[0], remove[1]):\n",
    "#             word_similarity[i][j] = 0\n",
    "#             word_similarity[j][i] = 0\n",
    "\n",
    "#         word_similarity = word_similarity > cutoff\n",
    "#         word_similarity = word_similarity.astype(int)\n",
    "#         np.fill_diagonal(word_similarity, 0)\n",
    "\n",
    "#         inds = np.where(word_similarity==1)\n",
    "#         embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "#         for i, j in zip(inds[0], inds[1]):\n",
    "#             embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "#         similar_words_bert.append(embeds)\n",
    "    \n",
    "#     idx = deepcopy(idx_copy)\n",
    "    \n",
    "#     print(idx)\n",
    "    \n",
    "#     for one_attentions in all_attentions[0].detach().numpy():\n",
    "\n",
    "#         one_side_edges = np.argmax(one_attentions[9], axis=1) #taking 9 layer of attention\n",
    "#         embeds = {words.index(j):[] for j in tokenized_text[idx]}\n",
    "\n",
    "#         for j, i in enumerate(one_side_edges[:pad_length[idx]]):\n",
    "#             if i < pad_length[idx]:\n",
    "#                 embeds[words.index(tokenized_text[idx][i])] += [words.index(tokenized_text[idx][j])]\n",
    "#         similar_words_bert_attention.append(embeds)\n",
    "#         idx += 1\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert' + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "# pickle.dump(similar_words_bert, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) +\"_\" + 'bert_attention'+ \".pickle\",\"wb\")\n",
    "# pickle.dump(similar_words_bert_attention, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### POS\n",
    "#         pp = np.array([i[1] for i in nltk.pos_tag(words)])\n",
    "#         pp[pp=='JJ'] = 1\n",
    "#         pp[pp=='JJR'] = 1\n",
    "#         pp[pp=='JJS'] = 1\n",
    "#         pp[pp=='NN'] = 1\n",
    "#         pp[pp=='NNS'] = 1\n",
    "#         pp[pp=='NNP'] = 1\n",
    "#         pp[pp=='NNPS'] = 1\n",
    "#         pp[pp!='1'] = 0\n",
    "#         pp = pp.astype(int)\n",
    "\n",
    "#         wordOccuranceMatrixBinary[:, np.where(pp!=1)[0]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordOccuranceMatrixBinary[0].sum()\n",
    "\n",
    "# np.sum(wordOccuranceMatrixBinary)\n",
    "\n",
    "# Counter(np.array([i[1] for i in nltk.pos_tag(words)]))\n",
    "\n",
    "# pp.sum()\n",
    "\n",
    "# np.where(pp!=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for embedding_name in ['bert', 'elmo']:\n",
    "#     for cutoff in cutoffs:\n",
    "#         print(embedding_name, cutoff)\n",
    "#         pool = multiprocessing.Pool(n_cores)\n",
    "#         similar_words = pool.map(get_edges_transformers, dataset.text.tolist())\n",
    "#         pool.close()\n",
    "#         pickle_out = open(\"resources/\"+ dataset_name + \"_\" + str(n_docs) + \"_\" + embedding_name + \"_\" + str(cutoff) + \".pickle\",\"wb\")\n",
    "#         pickle.dump(similar_words, pickle_out)\n",
    "#         pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pd = pd.apply(lambda x: convert_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_df(df):\n",
    "#     df['text'] = preprocess(df['reviewText'])\n",
    "    \n",
    "# #     pool = multiprocessing.Pool(n_cores)\n",
    "# #     df['cleaned'] = pool.map(process_l, df['text'].tolist())\n",
    "# #     pool.close()\n",
    "    \n",
    "# #     df['text'] = df['cleaned'].apply(lambda x: \" \".join(x))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [item for sublist in dataset['cleaned'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Counter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l(s):\n",
    "#     return [i.lemma_ for i in sp(s) if i.lemma_ not in '-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = dataset['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(n_cores)\n",
    "# processed_l = pool.map(process_l, l)\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(sampler, \"resources/sampler_20iter_0.5_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"resources/amazon_muiscal_glove_0.4.pickle\",\"wb\")\n",
    "# pickle.dump(similar_words, pickle_out)\n",
    "# pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eljst",
   "language": "python",
   "name": "eljst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
